{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14191cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# git commits\n",
    "gitCommitsdf = spark.read.csv('git-log-all.csv', header=True)\n",
    "#gitCommitsdf.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1249d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only required columns. i.e commit,author,date,insertions,deletions\n",
    "unique_git_commit_with_required_cols_df = gitCommitsdf.select(\"commit\",\"author\",\"date\",\"insertions\",\"deletions\")\n",
    "#git_commit_with_required_cols_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5164c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates \n",
    "# quesiton : does resuffle happens internally? I guess it does not happen in just one partition.[inefficient]\n",
    "#unique_git_commit_with_required_cols_df = git_commit_with_required_cols_df.dropDuplicates(subset = ['commit'])\n",
    "#print('Original record count = ', git_commit_with_required_cols_df.count())\n",
    "#print('Dropped record count = ', unique_git_commit_with_required_cols_df.count())\n",
    "#unique_git_commit_with_required_cols_df.filter(unique_git_commit_with_required_cols_df.author==\"Mohammed@talentica-all.com\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03d80d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert date to bit integer\n",
    "#Fri Mar 12 00:00:43 2021 +0530\n",
    "def binary_rep(datetimestamp):\n",
    "    loc = int(datetimestamp.split(\" \")[2])-1\n",
    "    return 1<<loc\n",
    "#print(binary_rep(\"Fri Mar 12 00:00:43 2021 +0530\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f4ed93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# month field funtion\n",
    "#Fri Mar 12 00:00:43 2021 +0530\n",
    "def get_month_year(datetimestamp):\n",
    "    day = 1\n",
    "    datetimestampArray = datetimestamp.split(\" \")\n",
    "    month = datetimestampArray[1]\n",
    "    year = datetimestampArray[4]\n",
    "    return \"0{fday}-{fmonth}-{fyear}\".format(fday = day, fmonth = month,fyear=year)\n",
    "#print(get_month_year(\"Fri Mar 12 00:00:43 2021 +0530\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34d12dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import IntegerType,StringType\n",
    "binary_rep_udf = udf(lambda d:binary_rep(d),IntegerType())\n",
    "get_month_year_udf = udf(lambda d:get_month_year(d),StringType())\n",
    "# transformation to be done on the date column\n",
    "#unique_git_commit_with_required_cols_df.take(5)\n",
    "trans_unique_git_commit_with_required_cols_df = unique_git_commit_with_required_cols_df.select(col(\"author\"),\\\n",
    "                                                                                               col(\"insertions\"),\\\n",
    "                                                                                               col(\"deletions\"),\\\n",
    "                                                                                               binary_rep_udf(col(\"date\")).alias(\"bday\"),\\\n",
    "                                                                                               get_month_year_udf(col(\"date\")).alias(\"month\"))                                                                                                                                                                                      \n",
    "#trans_unique_git_commit_with_required_cols_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e517f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read email aliases\n",
    "email_alias_df = spark.read.json(\"emailAliases.json\")\n",
    "required_email_alias_df = email_alias_df.select(\"alias\",\"email\")\n",
    "#print(emailAliasdf)\n",
    "#required_email_alias_df.show(truncate=False)\n",
    "#remove duplicates\n",
    "#unique_required_email_alias_df = required_email_alias_df.drop_duplicates(subset=['email'])\n",
    "#print(unique_required_email_alias_df.count() == required_email_alias_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4961a182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+---------+-----------+\n",
      "|               alias|insertions|    bday|deletions|      month|\n",
      "+--------------------+----------+--------+---------+-----------+\n",
      "|rakeshp@talentica...|        63|    2048|        2|01-Mar-2021|\n",
      "|rakeshp@talentica...|        63|    2048|        2|01-Mar-2021|\n",
      "|rakeshp@talentica...|        63|    2048|        2|01-Mar-2021|\n",
      "|rakeshp@talentica...|        63|    2048|        2|01-Mar-2021|\n",
      "|rakeshp@talentica...|        63|    2048|        2|01-Mar-2021|\n",
      "|rakeshp@talentica...|        63|    2048|        2|01-Mar-2021|\n",
      "|rakeshp@talentica...|        63|    2048|        2|01-Mar-2021|\n",
      "|Sagar.rajak@talen...|         2|     256|        2|01-Mar-2021|\n",
      "|Sagar.rajak@talen...|         2|     256|        3|01-Mar-2021|\n",
      "|Sagar.rajak@talen...|       174|     128|      146|01-Mar-2021|\n",
      "|Sagar.rajak@talen...|       174|     128|      146|01-Mar-2021|\n",
      "|Sagar.rajak@talen...|       174|     128|      146|01-Mar-2021|\n",
      "|rakeshp@talentica...|       117|       2|        0|01-Mar-2021|\n",
      "|rakeshp@talentica...|       117|       2|        0|01-Mar-2021|\n",
      "|rakeshp@talentica...|       117|       2|        0|01-Mar-2021|\n",
      "|rakeshp@talentica...|        16|16777216|        3|01-Feb-2021|\n",
      "|Sagar.rajak@talen...|        69| 8388608|       15|01-Feb-2021|\n",
      "|rakeshp@talentica...|         8| 8388608|       13|01-Feb-2021|\n",
      "|Sagar.rajak@talen...|         2| 4194304|        2|01-Feb-2021|\n",
      "|Sagar.rajak@talen...|         2| 4194304|        2|01-Feb-2021|\n",
      "+--------------------+----------+--------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_commit_df = trans_unique_git_commit_with_required_cols_df.join(required_email_alias_df,\\\n",
    "                                                  trans_unique_git_commit_with_required_cols_df.author == required_email_alias_df.email,\\\n",
    "                                                  'left')\n",
    "final_df = joined_commit_df.select(\"alias\",\"insertions\",\"bday\",\"deletions\",\"month\")\n",
    "final_df.show()\n",
    "#print(final_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4854d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "1.Create a user defined aggregate function. The problem is that you will need to write the user defined aggregate function in scala and wrap it to use in python.\n",
    "2.(inefficient)You can use the collect_list function to collect all values to a list and then write a UDF to combine them.\n",
    "3.(inefficient)You can move to RDD and use aggregate or aggregate by key.\n",
    "sum(\"insertions\",\"deletions\").show()\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import sum,avg,collect_list\n",
    "df = final_df.withColumn(\"insertions\",final_df.insertions.cast('int')).\\\n",
    "    withColumn(\"deletions\",final_df.deletions.cast('int')).\\\n",
    "    groupBy(\"alias\",\"month\").agg(sum(\"deletions\").alias(\"deletions\"),\\\n",
    "                                 sum(\"insertions\").alias(\"insertions\"),\\\n",
    "                                 collect_list(\"bday\").alias(\"days\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be1f28f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary days merge function\n",
    "from datetime import datetime\n",
    "def calculate_work_day(binary_rep_day,month):\n",
    "    dayOfWeek = datetime.strptime(month,\"%d-%b-%Y\").weekday()\n",
    "    dist=-1\n",
    "    total_days=0\n",
    "    for shift in range(0,31):\n",
    "        pos_b = 1 << shift\n",
    "        currdayOfWeek = (dayOfWeek + shift) % 7\n",
    "        if(currdayOfWeek == 6 or currdayOfWeek == 7 ):\n",
    "            continue\n",
    "        if dist != -1:\n",
    "            dist = dist+1\n",
    "        if(binary_rep_day & pos_b):\n",
    "            if dist == -1:\n",
    "                dist = 0\n",
    "            else:\n",
    "                if(dist <= 3):\n",
    "                    total_days = total_days + dist\n",
    "                dist = 0\n",
    "    return total_days\n",
    "#print(calculate_work_day([32,16,2,64],\"01-Oct-2020\"))\n",
    "def merge(daylist):\n",
    "    result = 0\n",
    "    for day in daylist:\n",
    "        result = result | day\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1b726ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_udf = udf(lambda d:merge(d),IntegerType())\n",
    "intermediate_result_df = df.select(col(\"alias\"),\\\n",
    "                                  col(\"month\"),\\\n",
    "                                  merge_udf(\"days\").alias(\"days\"),\\\n",
    "                                  col(\"insertions\"),\\\n",
    "                                  col(\"deletions\"))\n",
    "#intermediate_result_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edfebd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_work_day_udf = udf(lambda d,m:calculate_work_day(d,m),IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c1379f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "result_df = intermediate_result_df.select(col(\"alias\"),\\\n",
    "                                  col(\"month\"),\\\n",
    "                                  calculate_work_day_udf(\"days\",\"month\").alias(\"days\"),\\\n",
    "                                  col(\"insertions\"),\\\n",
    "                                  col(\"deletions\"))\n",
    "#result_df.show(truncate=False)\n",
    "#result_df.write.format(\"json\").mode(\"overwrite\").save(\"results.json\")\n",
    "print(result_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d06ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
